{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20180930 Update\n",
    "\n",
    "- reorg codes\n",
    "\t- flex\n",
    "\t- clearness\n",
    "\t- we should memic the strucutr of DiDi's OD flow prediction model code:\n",
    "\t\t- one function -> build model (different model strucutre)\n",
    "\t\t- one function -> train the model (specify small/large UCF or other dataset)\n",
    "\t\t- oen function -> inquiry on UCF\n",
    "\t\t- one function get accuracy description and save to txt\n",
    "- run experiments\n",
    "\t- DTW vs. simple matching (need 2 differnt length of video in inquiry set)\n",
    "\t- ConvLSTM vs. LSTM vs. Conv2D (sort of frame based VGG ) vs. (VidSig) (Non-DL method)\n",
    "\n",
    "# A simple test for inquiry model\n",
    "\n",
    "\n",
    "Here is the setting:\n",
    "\n",
    "- My thought: Yintai Ma [09:23] \n",
    "  - Suppose I have a video X of 100 frames, denoting as X[0] to X[99]. We then split it to multiple clips without overlap. If then length of each clip is 5 frames, then we come up with a sequences of clips X[0:5],X[5:10],...,X[95:100]. We use the encoder to transfer the sequences of clips to sequences of embeddeds, reads y[0],y[1],...,y[20]. Now we randomly pick some frames from X as validation clip and transfer it into validation embedded y_hat. We want to see how y_hat is matching to the sequences of y[0]...y[20].\n",
    "\n",
    "- Diego:\n",
    "  - You have video X. Randomly pick  sequences of frames (non overlapping). Say x[4:10], x[34:37],x[85:95]. Now concatenate them into a single video. This is now your query. From here, create embedding sequence y[0],â€¦y[K]. Now do DTW of y against your encoded sequences of the videos in the database.\n",
    "  - This is subject to experimentation. I agree that there should be overlap. Overlap by half.\n",
    "\n",
    "- My thought: There are many variations for the implementation:\n",
    "  1. we keep overlaps for y. We transfer X[0:5],X[1:6],....X[94:99],X[95:100] into y[0]...y[100]. Now we want to see how y_hat is matching to y[].\n",
    "  2. When we pick some frames from X to composite validation clips, do we always pick 5 consecutive frames? Should we ever transfer X[0]+X[2:6] into y_hat?\n",
    "  3. How we match y_hat to y[] if y_hat is also a sequences? I think this is where DTW comes in right? If y_hat is just one embedded, then what we need is basically a simple comparison between the distance of two embedded. However, if y_hat is a sequences, say it has y_hat[0] and y_hat[1], then we will need to use DTW to consider the case where both y_hat[0] matches to y[0] and y_hat[1] matches to y[3] are the best query retrieve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab.analytics.northwestern.edu/yma/.conda/envs/dl/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/lab.analytics.northwestern.edu/yma/.conda/envs/dl/lib/python2.7/site-packages/scipy/optimize/_minimize.py:32: ImportWarning: Not importing directory '/home/lab.analytics.northwestern.edu/yma/.conda/envs/dl/lib/python2.7/site-packages/scipy/optimize/lbfgsb': missing __init__.py\n",
      "  from .lbfgsb import _minimize_lbfgsb\n",
      "/home/lab.analytics.northwestern.edu/yma/.conda/envs/dl/lib/python2.7/site-packages/scipy/spatial/__init__.py:95: ImportWarning: Not importing directory '/home/lab.analytics.northwestern.edu/yma/.conda/envs/dl/lib/python2.7/site-packages/scipy/spatial/qhull': missing __init__.py\n",
      "  from .qhull import *\n"
     ]
    }
   ],
   "source": [
    "import sys,os,os.path\n",
    "sys.path.append(os.path.expanduser('/home/lab.analytics.northwestern.edu/yma/git/videodl/seq_inquiry'))\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "# matplotlib.use('Agg')\n",
    "import pylab as plt\n",
    "# from scipy.misc import toimage\n",
    "import pickle\n",
    "\n",
    "import imageio\n",
    "import cv2\n",
    "import numpy as np\n",
    "import numpngw\n",
    "import pandas as pd \n",
    "\n",
    "from IPython.display import HTML\n",
    "import random\n",
    "\n",
    "from mymodels import *\n",
    "from Video2videoInquiry import *\n",
    "\n",
    "import data_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab.analytics.northwestern.edu/yma/.conda/envs/dl/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py:539: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  return np.fromstring(tensor.tensor_content, dtype=dtype).reshape(shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Encoded embedding size: ', TensorShape([Dimension(None), Dimension(None), Dimension(7), Dimension(7), Dimension(8)]))\n",
      "--- Defining Decoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mymodels.py:208: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n",
      "  encoder = Model(output=encoded,input=inputs)\n",
      "mymodels.py:237: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ti...)`\n",
      "  autoencoder = Model(output=deconved,input=inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finish Compile and Plot Model ---\n",
      "('Encoded embedding size: ', TensorShape([Dimension(None), Dimension(None), Dimension(7), Dimension(7), Dimension(8)]))\n",
      "--- Defining Decoder ---\n",
      "--- Finish Compile and Plot Model ---\n",
      "('Encoded embedding size: ', TensorShape([Dimension(None), Dimension(None), Dimension(400)]))\n",
      "--- Defining Decoder ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mymodels.py:318: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"bn...)`\n",
      "  encoder = Model(output=encoded,input=inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finish Compile and Plot Model ---\n",
      "('Loading weights for setup:', 'simple_convlstm')\n",
      "('The output dimension of encoder is:', TensorShape([Dimension(None), Dimension(None), Dimension(7), Dimension(7), Dimension(8)]))\n",
      "('Loading weights for setup:', 'simple_conv')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer #14 (named \"time_distributed_12\"), weight <tf.Variable 'time_distributed_12/kernel:0' shape=(3, 3, 3, 64) dtype=float32_ref> has shape (3, 3, 3, 64), but the saved weight has shape (64, 8, 3, 3).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7c66ab8c5db6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading weights for setup:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"setup\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mmodel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_file\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The output dimension of encoder is:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lab.analytics.northwestern.edu/yma/.conda/envs/dl/lib/python2.7/site-packages/keras/engine/network.pyc\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 saving.load_weights_from_hdf5_group_by_name(\n\u001b[1;32m   1162\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_mismatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m                     reshape=reshape)\n\u001b[0m\u001b[1;32m   1164\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m                 saving.load_weights_from_hdf5_group(\n",
      "\u001b[0;32m/home/lab.analytics.northwestern.edu/yma/.conda/envs/dl/lib/python2.7/site-packages/keras/engine/saving.pyc\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group_by_name\u001b[0;34m(f, layers, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                                          \u001b[0;34m' has shape {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                                          \u001b[0;34m', but the saved weight has shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m                                          str(weight_values[i].shape) + '.')\n\u001b[0m\u001b[1;32m   1150\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                     weight_value_tuples.append((symbolic_weights[i],\n",
      "\u001b[0;31mValueError\u001b[0m: Layer #14 (named \"time_distributed_12\"), weight <tf.Variable 'time_distributed_12/kernel:0' shape=(3, 3, 3, 64) dtype=float32_ref> has shape (3, 3, 3, 64), but the saved weight has shape (64, 8, 3, 3)."
     ]
    }
   ],
   "source": [
    "LOAD_DATABASE_FROM_PKL = False\n",
    "\n",
    "# import conv_ae_config as config\n",
    "# model_file = \"ucf_vgg16_seq3_convlstm.001-0.0689.hdf5\" # encoding filter = 8\n",
    "# encoder, autoencoder =  ConvAutoEncoder(sequenceLength = config.sequenceLength)\n",
    "# database_file = \"/scratch/yma/data/inq_encoded_class100_video500_conv_convlstm_8.pkl\"\n",
    "\n",
    "import simple_convlstm_ae_config as config_simple_convlstm\n",
    "model_file_simple_convlstm = \"ucf_seq3_simple_convlstm.003-0.0284.hdf5\" # encoding filter = 8\n",
    "encoder_simple_convlstm, autoencoder_simple_convlstm =  SimpleConvLstmAutoEncoder(sequenceLength = config_simple_convlstm.sequenceLength)\n",
    "database_file_simple_convlstm = \"/scratch/yma/data/inq_encoded_class100_video500_simple_convlstm_8.pkl\"\n",
    "# database_file_simple_convlstm = \"/scratch/yma/data/inq_encoded_class100_video5000_simple_convlstm_8.pkl\"\n",
    "\n",
    "import simple_conv_ae_config as config_simple_conv\n",
    "# model_file = \"ucf_seq3_simple_conv.001-0.0231.hdf5\" # encoding filter = 8\n",
    "model_file_simple_conv = \"ucf_seq3_simple_conv.014-0.0114.hdf5\" # encoding filter = 8\n",
    "encoder_simple_conv, autoencoder_simple_conv =  SimpleConvAutoEncoder(sequenceLength = config_simple_conv.sequenceLength)\n",
    "database_file_simple_conv = \"/scratch/yma/data/inq_encoded_class100_video500_simple_conv_8.pkl\"\n",
    "# database_file_simple_conv = \"/scratch/yma/data/inq_encoded_class100_video5000_simple_conv_8.pkl\"\n",
    "\n",
    "import simple_lstm_ae_config as config_simple_lstm\n",
    "model_file_simple_lstm = \"ucf_seq3_simple_lstm.002-0.0726.hdf5\" # encoding filter = 8\n",
    "encoder_simple_lstm, autoencoder_simple_lstm =  SimpleLstmAutoEncoder(sequenceLength = config_simple_lstm.sequenceLength)\n",
    "database_file_simple_lstm = \"/scratch/yma/data/inq_encoded_class100_video500_simple_lstm_400.pkl\"\n",
    "# database_file_simple_lstm = \"/scratch/yma/data/inq_encoded_class100_video5000_simple_lstm_400.pkl\"\n",
    "\n",
    "models = [\n",
    "{\"setup\":\"simple_convlstm\",\n",
    "\"model_file\":model_file_simple_convlstm, \n",
    "\"encoder\":encoder_simple_convlstm,\n",
    "\"database_file\":database_file_simple_convlstm,\n",
    " \"config\":config_simple_convlstm\n",
    "},\n",
    "{\n",
    "\"setup\":\"simple_conv\",\n",
    "\"model_file\":model_file_simple_conv, \n",
    "\"encoder\":encoder_simple_conv,\n",
    "\"database_file\":database_file_simple_conv,\n",
    " \"config\":config_simple_conv},\n",
    "{\n",
    "\"setup\":\"simple_lstm\",\n",
    "\"model_file\":model_file_simple_lstm, \n",
    "\"encoder\":encoder_simple_lstm,\n",
    "\"database_file\":database_file_simple_lstm,\n",
    " \"config\":config_simple_lstm}\n",
    "]\n",
    "\n",
    "model_dir = \"/home/lab.analytics.northwestern.edu/yma/git/data/checkpoints/\"\n",
    "data_path = \"/scratch/yma/git/five-video-classification-methods/data\"\n",
    "\n",
    "for model_dict in models:\n",
    "    print(\"Loading weights for setup:\", model_dict[\"setup\"])\n",
    "    model_file = os.path.join(model_dir, model_dict[\"model_file\"])\n",
    "    model_dict[\"encoder\"].load_weights(model_file, by_name=True)\n",
    "    print(\"The output dimension of encoder is:\", model_dict[\"encoder\"].output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LENGTH = config.sequenceLength\n",
    "# the number of frames in each clip\n",
    "\n",
    "# N_database = 500\n",
    "# N_database = 5\n",
    "# N_database = 50\n",
    "\n",
    "inq_length = 4\n",
    "# the number of clips in the inquiry, no overlap\n",
    "\n",
    "DATASET_CLASS_LIMIT = 100\n",
    "# number of class is the dataset\n",
    "\n",
    "DATASET_VIDEO_IN_CLASS_LIMIT = 5\n",
    "\n",
    "FLAG_RANDOM_CLASS = True\n",
    "# whether randomly pick classes in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for model_dict in models:\n",
    "    encoder = model_dict[\"encoder\"]\n",
    "    database_file = model_dict[\"database_file\"]\n",
    "    config = model_dict[\"config\"]\n",
    "\n",
    "    model_dict[\"data\"] = initilize(encoder, data_path = data_path, \n",
    "                     seq_length = config.sequenceLength, \n",
    "                     class_limit = DATASET_CLASS_LIMIT, \n",
    "                     num_video_in_each_class = DATASET_VIDEO_IN_CLASS_LIMIT,\n",
    "                     random_class=FLAG_RANDOM_CLASS)\n",
    "\n",
    "    if LOAD_DATABASE_FROM_PKL:\n",
    "        with open(database_file, 'rb') as f:\n",
    "            model_dict[\"database\"] = pickle.load(f)\n",
    "    else:\n",
    "        model_dict[\"database\"] = create_database(encoder, data, \n",
    "                                   seq_length = config.sequenceLength, \n",
    "                                   class_limit = DATASET_CLASS_LIMIT, \n",
    "                                   num_video_in_each_class = DATASET_VIDEO_IN_CLASS_LIMIT)\n",
    "        # DATATYPE EXAMPLE: database.append((seqY, smp[2]))\n",
    "        with open(database_file, 'wb') as f:\n",
    "            pickle.dump(model_dict[\"database\"], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inqs = get_inquiry(data, if_random = True)\n",
    "# seqY = inqs[1]\n",
    "\n",
    "database = models[0][\"database\"]\n",
    "data = models[0][\"data\"]\n",
    "config = models[0][\"config\"]\n",
    "\n",
    "inq_dict, inq_result = inquiry_in_database(data, database, config, inq_length = inq_length , match_method = \"dtw\")\n",
    "show_inquriy_stats(inqs, inq_result, show_top_limit = 5)\n",
    "\n",
    "print(\"*\"*20)\n",
    "print(\"*\"*20)\n",
    "print(\"*\"*20)\n",
    "\n",
    "inq_dict, inq_result = inquiry_in_database(data, database, config, inq_length = inq_length, match_method = \"naive\", inq_dict = inq_dict)\n",
    "show_inquriy_stats(inqs, inq_result, show_top_limit = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (top_cat_same,top_cat_same_hit, Nth_score_avg, Hit_itself_avg) = multiple_test(data, run_times=100, if_itself=False)\n",
    "\n",
    "res_dict = multiple_test(models, test_run_for_each_database=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
